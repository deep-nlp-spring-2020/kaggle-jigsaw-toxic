{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Toxic bert_baseline.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bszMWxmI23oZ",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "sIlYMZYd23oa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "878527fe-fa8c-4b00-9591-3e1c04b74b24"
      },
      "source": [
        "!pip install transformers\n",
        "#VERSION = \"20200325\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "#!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "#!python pytorch-xla-env-setup.py --version $VERSION\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, time\n",
        "import pandas as pd\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZeG-o9Q23oj",
        "colab_type": "text"
      },
      "source": [
        "# Set global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "LyAzUSzn23ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 42\n",
        "MAX_LENGTH = 128\n",
        "EPOCHS = 30\n",
        "GPU_USE = True\n",
        "PRINT_EVERY = 100\n",
        "\n",
        "DATA_PATH =  \"./drive/My Drive/toxic_jigsaw/\"#toxic_data\n",
        "CHECKPOINT_PATH = 'best_model.dat'\n",
        "SST_TRAIN = \"jigsaw_train_merged_shortest.csv\"#\"jigsaw_train_merged.csv\"#\"jigsaw-toxic-comment-train.csv\"\n",
        "SST_VALID = \"validation.csv\"\n",
        "SST_TEST = \"test.csv\"\n",
        "\n",
        "SENTENCE_LABEL = 'comment_text'#'sentence' # comment_text\n",
        "TEST_LABEL = 'content'\n",
        "TARGET_LABEL = 'toxic' # toxic\n",
        "\n",
        "TOKENIZER_CLS=BertTokenizer\n",
        "MODEL_CLS=BertModel\n",
        "SHORTCUT_NAME='bert-base-multilingual-cased'"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6Hg1o7L23or",
        "colab_type": "text"
      },
      "source": [
        "# Fix seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLdSdRhf23os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHgT_h6v23oy",
        "colab_type": "text"
      },
      "source": [
        "# Examples\n",
        "\n",
        "Load and look at examples from [our first competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/). These are comments from Wikipedia with a variety of annotations (toxic, obscene, threat, etc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PkjUydz23oz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "outputId": "37bb7a34-c144-4cd0-a980-a8bd062f126b"
      },
      "source": [
        "#train = pd.read_csv(os.path.join(DATA_PATH, SST_TRAIN))\n",
        "#valid = pd.read_csv(os.path.join(DATA_PATH, SST_VALID))\n",
        "test = pd.read_csv(os.path.join(DATA_PATH, SST_TEST))\n",
        "test.head()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>content</th>\n",
              "      <th>lang</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Doctor Who adlı viki başlığına 12. doctor olar...</td>\n",
              "      <td>tr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Вполне возможно, но я пока не вижу необходимо...</td>\n",
              "      <td>ru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Quindi tu sei uno di quelli   conservativi  , ...</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Malesef gerçekleştirilmedi ancak şöyle bir şey...</td>\n",
              "      <td>tr</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>:Resim:Seldabagcan.jpg resminde kaynak sorunu ...</td>\n",
              "      <td>tr</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                            content lang\n",
              "0   0  Doctor Who adlı viki başlığına 12. doctor olar...   tr\n",
              "1   1   Вполне возможно, но я пока не вижу необходимо...   ru\n",
              "2   2  Quindi tu sei uno di quelli   conservativi  , ...   it\n",
              "3   3  Malesef gerçekleştirilmedi ancak şöyle bir şey...   tr\n",
              "4   4  :Resim:Seldabagcan.jpg resminde kaynak sorunu ...   tr"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LED9tTXy23o6",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQn0LvjG23o7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"class ToxicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, filename, maxlen):\n",
        "        self.df = pd.read_csv(filename)\n",
        "        self.tokenizer = TOKENIZER_CLS.from_pretrained(SHORTCUT_NAME)\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #Selecting the sentence and label at the specified index in the data frame\n",
        "        sentence = self.df.loc[index, SENTENCE_LABEL]\n",
        "        label = self.df.loc[index, TARGET_LABEL]\n",
        "        \n",
        "        encoded = self.tokenizer.encode_plus(\n",
        "            sentence, \n",
        "            add_special_tokens=True, \n",
        "            max_length=MAX_LENGTH, \n",
        "            pad_to_max_length=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return encoded['input_ids'], encoded['attention_mask'], label\n",
        "\n",
        "\n",
        "class ToxicTestDataset(Dataset):\n",
        "\n",
        "    def __init__(self, filename, maxlen):\n",
        "        self.df = pd.read_csv(filename)\n",
        "        self.tokenizer = TOKENIZER_CLS.from_pretrained(SHORTCUT_NAME)\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #Selecting the sentence and label at the specified index in the data frame\n",
        "        sentence = self.df.loc[index, TEST_LABEL]\n",
        "        encoded = self.tokenizer.encode_plus(\n",
        "            sentence, \n",
        "            add_special_tokens=True, \n",
        "            max_length=MAX_LENGTH, \n",
        "            pad_to_max_length=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return encoded['input_ids'], encoded['attention_mask']\n",
        "\"\"\"\n",
        "class ToxicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, filename, maxlen):\n",
        "\n",
        "        #Store the contents of the file in a pandas dataframe\n",
        "        self.df = pd.read_csv(filename)\n",
        "\n",
        "        #Initialize the BERT tokenizer\n",
        "        self.tokenizer = TOKENIZER_CLS.from_pretrained(SHORTCUT_NAME)\n",
        "\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #Selecting the sentence and label at the specified index in the data frame\n",
        "        sentence = self.df.loc[index, SENTENCE_LABEL]\n",
        "        label = self.df.loc[index, TARGET_LABEL]\n",
        "        #Preprocessing the text to be suitable for BERT\n",
        "        tokens = self.tokenizer.tokenize(sentence) #Tokenize the sentence\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]'] #Insering the CLS and SEP token in the beginning and end of the sentence\n",
        "        if len(tokens) < self.maxlen:\n",
        "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] #Padding sentences\n",
        "        else:\n",
        "            tokens = tokens[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
        "\n",
        "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
        "        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n",
        "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
        "        attn_mask = (tokens_ids_tensor != 0).long()\n",
        "\n",
        "        return tokens_ids_tensor, attn_mask, label\n",
        "\n",
        "class ToxicTestDataset(Dataset):\n",
        "\n",
        "    def __init__(self, filename, maxlen):\n",
        "\n",
        "        #Store the contents of the file in a pandas dataframe\n",
        "        self.df = pd.read_csv(filename)\n",
        "\n",
        "        #Initialize the BERT tokenizer\n",
        "        self.tokenizer = TOKENIZER_CLS.from_pretrained(SHORTCUT_NAME)\n",
        "\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #Selecting the sentence and label at the specified index in the data frame\n",
        "        sentence = self.df.loc[index, TEST_LABEL]\n",
        "        #Preprocessing the text to be suitable for BERT\n",
        "        tokens = self.tokenizer.tokenize(sentence) #Tokenize the sentence\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]'] #Insering the CLS and SEP token in the beginning and end of the sentence\n",
        "        if len(tokens) < self.maxlen:\n",
        "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] #Padding sentences\n",
        "        else:\n",
        "            tokens = tokens[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
        "\n",
        "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
        "        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n",
        "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
        "        attn_mask = (tokens_ids_tensor != 0).long()\n",
        "        return tokens_ids_tensor, attn_mask"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g8w1w1E23pB",
        "colab_type": "text"
      },
      "source": [
        "# Model with Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eM04pRd23pC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, freeze_bert = True):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        #Instantiating BERT model object \n",
        "        self.bert_layer = MODEL_CLS.from_pretrained(SHORTCUT_NAME)\n",
        "        \n",
        "        #Freeze bert layers\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "        \n",
        "        #Classification layer\n",
        "        self.cls_layer = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, seq, attn_masks):\n",
        "        '''\n",
        "        Inputs:\n",
        "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
        "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
        "        '''\n",
        "\n",
        "        #Feeding the input to BERT model to obtain contextualized representations\n",
        "        cont_reps, _ = self.bert_layer(seq, attention_mask = attn_masks)\n",
        "\n",
        "        #Obtaining the representation of [CLS] head\n",
        "        cls_rep = cont_reps[:, 0]\n",
        "\n",
        "        #Feeding cls_rep to the classifier layer\n",
        "        logits = self.cls_layer(cls_rep)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btGK8nOL23pI",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-VznBC423pJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_accuracy_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "    return acc\n",
        "\n",
        "def get_auc_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "    try:\n",
        "      auc = roc_auc_score(labels.cpu().detach().numpy(), probs.squeeze().cpu().detach().numpy())\n",
        "    except ValueError:\n",
        "      auc = 0\n",
        "    return auc\n",
        "\n",
        "def evaluate(net, criterion, dataloader, gpu):\n",
        "    net.eval()\n",
        "\n",
        "    mean_acc, mean_auc, mean_loss = 0, 0, 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for seq, attn_masks, labels in dataloader:\n",
        "        seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
        "        #seq, attn_masks, labels = seq, attn_masks, labels\n",
        "        logits = net(seq, attn_masks)\n",
        "        mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
        "        mean_acc += get_accuracy_from_logits(logits, labels)\n",
        "        mean_auc += get_auc_from_logits(logits, labels)\n",
        "        count += 1\n",
        "\n",
        "    return mean_acc / count, mean_auc / count, mean_loss / count\n",
        "\n",
        "\n",
        "def predict_submit(net, test_loader):\n",
        "  net.eval()\n",
        "  with torch.no_grad():\n",
        "    result = torch.tensor([])\n",
        "    for seq, attn_masks in test_loader:\n",
        "      seq, attn_masks = seq.cuda(), attn_masks.cuda()\n",
        "      logits = net(seq, attn_masks)\n",
        "      probs = torch.sigmoid(logits.unsqueeze(-1)).squeeze().cpu().detach()\n",
        "      result = torch.cat((result, probs))\n",
        "  return result\n",
        "\n",
        "\n",
        "def train(net, criterion, opti, train_loader, val_loader, max_eps, gpu, print_every):\n",
        "    best_auc = 0\n",
        "    for ep in range(max_eps):\n",
        "        \n",
        "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
        "            #Clear gradients\n",
        "            opti.zero_grad()  \n",
        "            #Converting these to cuda tensors\n",
        "            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
        "            #seq, attn_masks, labels = seq, attn_masks, labels\n",
        "\n",
        "            #Obtaining the logits from the model\n",
        "            logits = net(seq, attn_masks)\n",
        "\n",
        "            #Computing loss\n",
        "            loss = criterion(logits.squeeze(-1), labels.float())\n",
        "\n",
        "            #Backpropagating the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            #Optimization step\n",
        "            opti.step()\n",
        "\n",
        "            if (it + 1) % print_every == 0:\n",
        "                #acc = get_accuracy_from_logits(logits, labels)\n",
        "                auc = get_auc_from_logits(logits, labels)\n",
        "                print(f\"Iteration {it+1} of epoch {ep+1} complete. Loss : {loss.item()} AUC : {auc}\")\n",
        "         \n",
        "        val_acc, val_auc, val_loss = evaluate(net, criterion, val_loader, gpu)\n",
        "        print(\"Epoch {} complete! Validation AUC : {}, Validation Loss : {}\".format(ep, val_auc, val_loss))\n",
        "        if val_auc > best_auc:\n",
        "            print(\"Best validation accuracy improved from {} to {}, saving model...\".format(best_auc, val_auc))\n",
        "            predicted = predict_submit(net, test_loader).numpy()\n",
        "            subm = pd.read_csv(os.path.join(DATA_PATH, \"sample_submission.csv\"))\n",
        "            subm['toxic'] = predicted\n",
        "            subm.to_csv(os.path.join(DATA_PATH, f'submission_ep_{ep}_val_{int(val_auc*100)}.csv'), index = None)\n",
        "            best_auc = val_auc\n",
        "            torch.save(net.state_dict(), os.path.join(DATA_PATH, CHECKPOINT_PATH))"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp5h9vFU23pM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "outputId": "767c7f82-ffcf-492e-fd56-8d5bbaa492c3"
      },
      "source": [
        "#Creating instances of training and validation set\n",
        "train_set = ToxicDataset(filename = os.path.join(DATA_PATH, SST_TRAIN), maxlen = MAX_LENGTH)\n",
        "val_set = ToxicDataset(filename = os.path.join(DATA_PATH, SST_VALID), maxlen = MAX_LENGTH)\n",
        "test_set = ToxicTestDataset(filename = os.path.join(DATA_PATH, SST_TEST), maxlen = MAX_LENGTH)\n",
        "\n",
        "#Creating intsances of training and validation dataloaders\n",
        "train_loader = DataLoader(train_set, batch_size = 64, num_workers = 5)\n",
        "val_loader = DataLoader(val_set, batch_size = 64, num_workers = 5)\n",
        "test_loader = DataLoader(test_set, batch_size = 64, num_workers = 5)\n",
        "\n",
        "net = SentimentClassifier(freeze_bert = True)\n",
        "net.cuda()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "#opti = optim.AdamW(net.parameters(), lr = 2e-5, weight_decay=0.01)\n",
        "opti = optim.Adam(net.parameters(), lr = 2e-5)\n",
        "\n",
        "train(net, criterion, opti, train_loader, val_loader, EPOCHS, GPU_USE, PRINT_EVERY)\n",
        "\n",
        "checkpoint = torch.load(CHECKPOINT_PATH, map_location=torch.device('cpu'))\n",
        "net.load_state_dict(checkpoint);\n",
        "\n",
        "checkpoint = None\n",
        "del checkpoint\n",
        "#predicted = predict_submit(net, test_loader).numpy()\n",
        "#print('AUCC', roc_auc_score(valid['label'].values, predicted))\n",
        "predicted = predict_submit(net, test_loader).numpy()\n",
        "subm = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))\n",
        "subm['toxic'] = predicted\n",
        "subm.to_csv('submission.csv', index = None)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 100 of epoch 1 complete. Loss : 0.5465466976165771 AUC : 0.6451612903225807\n",
            "Iteration 200 of epoch 1 complete. Loss : 0.5493518710136414 AUC : 0.6681749622926094\n",
            "Iteration 300 of epoch 1 complete. Loss : 0.48416227102279663 AUC : 0.696969696969697\n",
            "Iteration 400 of epoch 1 complete. Loss : 0.4484972357749939 AUC : 0.5647321428571428\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-7af15314b41b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mopti\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopti\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPU_USE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRINT_EVERY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINT_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-beaf14887d9c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, criterion, opti, train_loader, val_loader, max_eps, gpu, print_every)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mopti\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;31m#Converting these to cuda tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0mseq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m             \u001b[0;31m#seq, attn_masks, labels = seq, attn_masks, labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS3d8U0m23pQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}