{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "xlmroberta_baseline_mtalimanchuk.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bszMWxmI23oZ",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "sIlYMZYd23oa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "ce4ba788-8231-41c3-ab8c-74221808ea6b"
      },
      "source": [
        "!pip install transformers\n",
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from transformers import XLMRobertaModel, XLMRobertaTokenizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZeG-o9Q23oj",
        "colab_type": "text"
      },
      "source": [
        "# Set global variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "LyAzUSzn23ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 42\n",
        "MAX_LENGTH = 100\n",
        "DEVICE = \"cuda\"\n",
        "\n",
        "CHECKPOINT_PATH = 'best_model.dat'\n",
        "DATA_PATH =  \"jigsaw\"\n",
        "TRAIN_FILE = \"jigsaw-toxic-comment-train.csv\"\n",
        "VALID_FILE = \"validation.csv\"\n",
        "TEST_FILE = \"test.csv\"\n",
        "SAMPLE_SUBMISSION_FILE = \"sample_submission.csv\"\n",
        "SUBMISSION_FILE = \"submission.csv\"\n",
        "\n",
        "SENTENCE_LABEL = 'comment_text'\n",
        "TARGET_LABEL = 'toxic'\n",
        "TEST_LABEL = 'content'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w6Hg1o7L23or",
        "colab_type": "text"
      },
      "source": [
        "# Fix seed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLdSdRhf23os",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(SEED)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHgT_h6v23oy",
        "colab_type": "text"
      },
      "source": [
        "# Examples\n",
        "\n",
        "Load and look at examples from [our first competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/). These are comments from Wikipedia with a variety of annotations (toxic, obscene, threat, etc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PkjUydz23oz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train = pd.read_csv(os.path.join(DATA_PATH, TRAIN_FILE))\n",
        "# valid = pd.read_csv(os.path.join(DATA_PATH, VALID_FILE))\n",
        "# train.head()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LED9tTXy23o6",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQn0LvjG23o7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ToxicDataset(Dataset):\n",
        "\n",
        "    def __init__(self, filename, maxlen, tokenizer_cls, tokenizer_name):\n",
        "        self.df = pd.read_csv(filename)\n",
        "        self.tokenizer = tokenizer_cls.from_pretrained(tokenizer_name)\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #Selecting the sentence and label at the specified index in the data frame\n",
        "        sentence = self.df.loc[index, SENTENCE_LABEL]\n",
        "        label = self.df.loc[index, TARGET_LABEL]\n",
        "        #Preprocessing the text to be suitable for BERT\n",
        "        tokens = self.tokenizer.tokenize(sentence) #Tokenize the sentence\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]'] #Insering the CLS and SEP token in the beginning and end of the sentence\n",
        "        if len(tokens) < self.maxlen:\n",
        "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] #Padding sentences\n",
        "        else:\n",
        "            tokens = tokens[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
        "\n",
        "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
        "        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n",
        "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
        "        attn_mask = (tokens_ids_tensor != 0).long()\n",
        "\n",
        "        return tokens_ids_tensor, attn_mask, label\n",
        "\n",
        "\n",
        "class ToxicTestDataset(Dataset):\n",
        "\n",
        "    def __init__(self, filename, maxlen, tokenizer_cls, tokenizer_name):\n",
        "        self.df = pd.read_csv(filename)\n",
        "        self.tokenizer = tokenizer_cls.from_pretrained(tokenizer_name)\n",
        "        self.maxlen = maxlen\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        #Selecting the sentence and label at the specified index in the data frame\n",
        "        sentence = self.df.loc[index, TEST_LABEL]\n",
        "        #Preprocessing the text to be suitable for BERT\n",
        "        tokens = self.tokenizer.tokenize(sentence) #Tokenize the sentence\n",
        "        tokens = ['[CLS]'] + tokens + ['[SEP]'] #Insering the CLS and SEP token in the beginning and end of the sentence\n",
        "        if len(tokens) < self.maxlen:\n",
        "            tokens = tokens + ['[PAD]' for _ in range(self.maxlen - len(tokens))] #Padding sentences\n",
        "        else:\n",
        "            tokens = tokens[:self.maxlen-1] + ['[SEP]'] #Prunning the list to be of specified max length\n",
        "\n",
        "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens) #Obtaining the indices of the tokens in the BERT Vocabulary\n",
        "        tokens_ids_tensor = torch.tensor(tokens_ids) #Converting the list to a pytorch tensor\n",
        "        #Obtaining the attention mask i.e a tensor containing 1s for no padded tokens and 0s for padded ones\n",
        "        attn_mask = (tokens_ids_tensor != 0).long()\n",
        "        return tokens_ids_tensor, attn_mask"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g8w1w1E23pB",
        "colab_type": "text"
      },
      "source": [
        "# Model with Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eM04pRd23pC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ToxicClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, model_cls, model_name, linear_shape, freeze_bert=True):\n",
        "        super().__init__()\n",
        "        self.transformer_layer = model_cls.from_pretrained(model_name)\n",
        "\n",
        "        if freeze_bert:\n",
        "            for p in self.transformer_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "        \n",
        "        self.cls_layer = nn.Linear(*linear_shape)\n",
        "\n",
        "    def forward(self, seq, attn_masks):\n",
        "        '''\n",
        "        Inputs:\n",
        "            -seq : Tensor of shape [B, T] containing token ids of sequences\n",
        "            -attn_masks : Tensor of shape [B, T] containing attention masks to be used to avoid contibution of PAD tokens\n",
        "        '''\n",
        "\n",
        "        #Feeding the input to BERT model to obtain contextualized representations\n",
        "        cont_reps, _ = self.transformer_layer(seq, attention_mask = attn_masks)\n",
        "\n",
        "        #Obtaining the representation of [CLS] head\n",
        "        cls_rep = cont_reps[:, 0]\n",
        "\n",
        "        #Feeding cls_rep to the classifier layer\n",
        "        logits = self.cls_layer(cls_rep)\n",
        "\n",
        "        return logits\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btGK8nOL23pI",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-VznBC423pJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tensor2numpy(tensor):\n",
        "    return tensor.squeeze().cpu().detach().numpy()\n",
        "\n",
        "def get_roc_auc_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "\n",
        "    try:\n",
        "      roc_auc = roc_auc_score(tensor2numpy(labels), tensor2numpy(probs))\n",
        "    except:\n",
        "      roc_auc = 0\n",
        "\n",
        "    return roc_auc\n",
        "\n",
        "\n",
        "def get_acc_from_logits(logits, labels):\n",
        "    probs = torch.sigmoid(logits.unsqueeze(-1))\n",
        "\n",
        "    soft_probs = (probs > 0.5).long()\n",
        "    acc = (soft_probs.squeeze() == labels).float().mean()\n",
        "\n",
        "    return acc\n",
        "\n",
        "\n",
        "def evaluate(net, criterion, dataloader):\n",
        "    net.eval()\n",
        "\n",
        "    mean_roc_auc = mean_acc = mean_loss = 0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for seq, attn_masks, labels in dataloader:\n",
        "            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
        "            logits = net(seq, attn_masks)\n",
        "            mean_roc_auc += get_roc_auc_from_logits(logits, labels)\n",
        "            mean_acc += get_acc_from_logits(logits, labels)\n",
        "            mean_loss += criterion(logits.squeeze(-1), labels.float()).item()\n",
        "            count += 1\n",
        "\n",
        "    return (\n",
        "        mean_roc_auc / count,\n",
        "        mean_acc / count,\n",
        "        mean_loss / count\n",
        "    )\n",
        "\n",
        "\n",
        "def train(net, criterion, opti, train_loader, val_loader, max_eps, print_every):\n",
        "    best_roc_auc = 0\n",
        "\n",
        "    for ep in range(max_eps):\n",
        "        \n",
        "        for it, (seq, attn_masks, labels) in enumerate(train_loader):\n",
        "            #Clear gradients\n",
        "            opti.zero_grad()  \n",
        "            #Converting these to cuda tensors\n",
        "            seq, attn_masks, labels = seq.cuda(), attn_masks.cuda(), labels.cuda()\n",
        "\n",
        "            #Obtaining the logits from the model\n",
        "            logits = net(seq, attn_masks)\n",
        "\n",
        "            #Computing loss\n",
        "            loss = criterion(logits.squeeze(-1), labels.float())\n",
        "\n",
        "            #Backpropagating the gradients\n",
        "            loss.backward()\n",
        "\n",
        "            #Optimization step\n",
        "            opti.step()\n",
        "\n",
        "            if (it + 1) % print_every == 0:\n",
        "                roc_auc = get_roc_auc_from_logits(logits, labels)\n",
        "                acc = get_acc_from_logits(logits, labels)\n",
        "                print(\n",
        "                    f\"Iteration {it+1} of epoch {ep+1} complete. \"\n",
        "                    f\"Loss : {loss.item()} ROC AUC : {roc_auc} Accuracy : {acc}\"\n",
        "                )\n",
        "\n",
        "        val_roc_auc, val_acc, val_loss = evaluate(net, criterion, val_loader)\n",
        "        print(\n",
        "            f\"Epoch {ep} complete!\"\n",
        "            f\"Validation ROC AUC : {val_roc_auc}, \"\n",
        "            f\"Validation accuracy : {val_acc}, \"\n",
        "            f\"Validation Loss : {val_loss}\"\n",
        "        )\n",
        "\n",
        "        if val_roc_auc > best_roc_auc:\n",
        "            print(\n",
        "                f\"Best validation ROC AUC improved from {best_roc_auc} to {val_roc_auc}, saving model...\"\n",
        "            )\n",
        "            best_roc_auc = val_roc_auc\n",
        "            torch.save(net.state_dict(), CHECKPOINT_PATH)\n",
        "\n",
        "        yield ep, val_roc_auc, val_acc, val_loss\n",
        "\n",
        "\n",
        "def predict_submit(net, test_loader):\n",
        "    net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        result = torch.tensor([])\n",
        "        for seq, attn_masks in test_loader:\n",
        "            seq, attn_masks = seq.cuda(), attn_masks.cuda()\n",
        "            logits = net(seq, attn_masks)\n",
        "            probs = torch.sigmoid(logits.unsqueeze(-1)).squeeze().cpu().detach()\n",
        "            result = torch.cat((result, probs))\n",
        "\n",
        "    return result\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sp5h9vFU23pM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "fea95241-804d-4d73-d99a-97b6c022cdf9"
      },
      "source": [
        "MODEL = XLMRobertaModel\n",
        "TOKENIZER = XLMRobertaTokenizer\n",
        "TOKENIZER_NAME = MODEL_NAME = \"xlm-roberta-large\"\n",
        "LINEAR_LAYER_SHAPE = (1024, 1)  # (1024, 1) for XLMRoberta; (768, 1) for Bert\n",
        "BATCH_SIZE = 32\n",
        "LR = 1e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "EPOCHS = 15\n",
        "PRINT_EVERY = 100\n",
        "DATALOADER_WORKERS = 5\n",
        "\n",
        "#Creating instances of training and validation set\n",
        "train_set = ToxicDataset(os.path.join(DATA_PATH, TRAIN_FILE), MAX_LENGTH, TOKENIZER, TOKENIZER_NAME)\n",
        "val_set = ToxicDataset(os.path.join(DATA_PATH, VALID_FILE), MAX_LENGTH, TOKENIZER, TOKENIZER_NAME)\n",
        "test_set = ToxicTestDataset(os.path.join(DATA_PATH, TEST_FILE), MAX_LENGTH, TOKENIZER, TOKENIZER_NAME)\n",
        "\n",
        "#Creating intsances of training and validation dataloaders\n",
        "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, num_workers=DATALOADER_WORKERS)\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, num_workers=DATALOADER_WORKERS)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, num_workers=DATALOADER_WORKERS)\n",
        "\n",
        "net = ToxicClassifier(MODEL, MODEL_NAME, LINEAR_LAYER_SHAPE, freeze_bert=False)\n",
        "net.cuda()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "opti = optim.AdamW(net.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs_plot = []\n",
        "losses_plot = []\n",
        "roc_aucs_plot = []\n",
        "accs_plot = []\n",
        "\n",
        "try:\n",
        "  print(\"Started training...\\n\")\n",
        "  for epoch_n, roc_auc, acc, loss in train(\n",
        "      net, criterion, opti, train_loader, val_loader, EPOCHS, PRINT_EVERY\n",
        "  ):\n",
        "    clear_output()\n",
        "\n",
        "    epochs_plot.append(epoch_n + 1)\n",
        "    roc_aucs_plot.append(roc_auc)\n",
        "    accs_plot.append(acc)\n",
        "    losses_plot.append(loss)\n",
        "\n",
        "    plt.plot(epochs_plot, roc_aucs_plot, marker=\".\", label=\"ROC AUC\")\n",
        "    plt.plot(epochs_plot, accs_plot, marker=\".\", label=\"Accuracy\")\n",
        "    plt.plot(epochs_plot, losses_plot, marker=\".\", label=\"Loss\")\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.title(f\"Max ROC AUC = {max(roc_aucs_plot)}\")\n",
        "    plt.show()\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "  print(\"Training interrupted\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training interrupted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbCor6O9KmmP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "101a2d1d-47f8-4abe-afb7-c9e3afaee15e"
      },
      "source": [
        "checkpoint = torch.load(CHECKPOINT_PATH, map_location=torch.device('cpu'))\n",
        "net.load_state_dict(checkpoint)\n",
        "\n",
        "checkpoint = None\n",
        "del checkpoint\n",
        "predicted = predict_submit(net, test_loader).numpy()\n",
        "subm = pd.read_csv(os.path.join(DATA_PATH, SAMPLE_SUBMISSION_FILE))\n",
        "subm['toxic'] = predicted\n",
        "subm.to_csv(SUBMISSION_FILE, index = None)\n",
        "print(f\"Saved to {SUBMISSION_FILE}\")\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved to submission.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZP-NFxiTKLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}